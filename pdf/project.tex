\documentclass[10pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\usepackage[margin=2cm]{geometry}

\graphicspath{ {./images/} }

\title{Computational Physics Project \\Â Integrating Quantum Probability Densities}
\author{Tilman Roeder}
\date{\today}

% Uniform plots
\newcommand{\plot}[3]{\begin{figure}[htp]\centering\includegraphics[width=10cm]{#1}\caption{#2}\label{#3}\end{figure}}

% Abbreviations
\newcommand{\abbreviate}[3]{\newcommand{#1}{#3 \textit{(#2)}\renewcommand{#1}{#2}}}
\abbreviate{\apis}{APIS}{Adaptive Population Importance Sampling}
\abbreviate{\iid}{i.i.d.}{independent and identically distributed}
\abbreviate{\is}{IS}{Importance Sampling}


\newcommand{\final}{$I = 0.497661132 \pm 2\times10^{-9}$}

\begin{document}
\maketitle
\abstract{
  Using quadrature and Monte-Carlo integration schemes it is determined that
  $\int_0^2 \frac{1}{\sqrt{\pi}} e^{-x^2} dx \approx$ \final. We
  find that for this one dimensional problem, quadrature methods
  are favorable as they converge much more rapidly. For Monte-Carlo methods
  we find that the efficiency of methods matches theoretical expectations.

  Specifically, the convergence is $\sim \max(\frac fp)$ for Importance Sampling
  and slightly improves using adaptive methods.
}

\section{Introduction}
In this report we investigate methods for numerically computing the value of

\begin{equation}
\label{eq:target}
I = \int_a^b |\Psi(x)|^2 dx = \int_a^b \frac{1}{\sqrt{\pi}} e^{-x^2} dx,
\end{equation}
where we are interested in the case $a = 0$, $b = 2$.

$|\Psi(x)|^2$ is a normalized Gaussian probability density for which
very accurate approximations and tabulated values exist. However, for the purposes of this report we
treat the value of $I$ as unknown and investigate how to justify confidence in the
obtained results.

A direct comparison with tabulated values can be found in appendix \ref{app:cheat}.

\section{Algorithms}
\label{sec:algo}
We explore two general classes of algorithms: quadrature methods, and Monte-Carlo integration.

\subsection{Quadrature Methods}
  \subsubsection{Trapezoidal Rule}
  \label{sec:trap}
  Consider approximating the integrand $f(x)$ as a linear function going through $f(a)$, $f(b)$.
  Then, if $f$ is analytic, we have $f(x) = f_0 + f_0^\prime (x-x_0) + \mathcal{O}\left((x-x_0)^2\right)$
  and thus\footnotemark
  \begin{equation}
  \int_a^b f(x) dx =
  \int_a^b f(x_0) + \frac{f(b) - f(a)}{b-a} (x-a) + \mathcal{O}(x^2) dx =
  \frac{1}{2} h (f(a) + f(b)) + \mathcal{O}(h^3),
  \end{equation}
  where $h = b-a$ and $x_0 \in [a,b]$.

  \footnotetext{
    For analytic functions: $\forall a, b \in \mathbb{R} \exists x_0 \in [a, b]$ s.t.
    $f^\prime(x_0) = \frac{f(b)-f(a)}{b-a}$.
  }

  To compute the integral over a longer range we extend the rule by dividing the integration
  region into $N$ parts such that
  \begin{equation}
  \begin{split}
  I = \int_a^b f(x) dx & = \sum_{i=0}^{N-1} \int_{a+ih}^{a+(i+1)h} f(x) dx \\
  & = \sum_{i=0}^{N-1} \frac{1}{2} h (f(a+ih) + f(a+(i+1)h)) + \mathcal{O}(h^3) \\
  & = \frac{1}{2} h (f(a) + f(b)) + \left( \sum_{i=1}^{N-1} h f(a+ih) \right) + \mathcal{O}(h^2),
  \end{split}
  \end{equation}
  where $h = \frac{b-a}{N}$.

  Notice that given a desired accuracy $\epsilon$ in general $N \sim \frac{b-a}{\sqrt{\epsilon}}$
  function evaluations are necessary to determine the integral.

  Implementing this scheme requires $\mathcal{O}(N)$ time and $\mathcal{O}(1)$ space. However, making
  use of the strong support for parallel computing in modern CPUs and the Go programming language allows
  us to reduce the time complexity by a factor of $M$, where $M$ is the number of threads
  running concurrently\footnotemark.

  \footnotetext{This is true, since $M$ is limited by the number of cores in our processing unit. Given
  a theoretically infinite amount of truly concurrent threads we would want to choose a scheme that
  combines two numbers at a time, leading to $\log_2(N)$ threads and time complexity. It is clear
  that for any interesting integral this is limited by the number of processing cores available.}

  Further, we can implement a scheme to progressively increases the number of integration steps until
  a desired accuracy is reached. A straight-forward way to achieve this is to add mid-way points
  between the samples taken with every iteration:
  \begin{equation}
  \label{eq:trap-rec}
  I_m = \frac{I_{m-1}}{2} + \sum_i h_m f(x^{(m)}_i),
  \end{equation}
  where $m$ denotes the refinement step, $I_0 = h_0\frac{f(a) + f(b)}{2}$, $h_0 = b-a$,
  $h_m = \frac{h_{m-1}}{2}$, and $x^{(m)}_i$
  ranges over the subdivisions at the $m^{\text{th}}$ step.

  An efficient way to estimate the error after a given step, is to compute $\epsilon_m \approx |I_m - I_{m-1}|$
  \cite{nr}.

  This algorithm is implemented in \texttt{pkg/quad/trap.go}. The implementation parallelizes the computation
  of the summation in equation \ref{eq:trap-rec}, which is implemented in \texttt{pkg/quad/trap\_step.go}.
  The \texttt{quad} package defines a generic \texttt{Integral} interface, which the algorithm implements.

  The parallel algorithm computing the steps assumes that computing $f(x)$ is non-trivial, in the sense
  that is takes longer to compute than adding the result to the integral total. Thus, the
  algorithm has a worst case runtime of $\mathcal{O}(N)$, and best case runtime of $\mathcal{O}(\frac NM)$.

  \subsubsection{Simpson Rule}
  This quadrature method is closely related to the Trapezoidal Rule\footnotemark. Consider
  $f(x)$ to be approximated by a quadratic polynomial $q(x)$, such that $f(a) = q(a)$,
  $f(\frac{a+b}{2}) = q(\frac{a+b}{2})$, and $f(b) = q(b)$.

  \footnotetext{Indeed, we may understand both as special cases of the Runge-Kutta scheme for integrating
  ordinary first order differential equations\cite{nr}.}

  Now take the following Ansatz:
  \begin{equation}
  \frac{1}{b-a} \int_a^b q(x) dx = \alpha q(a) + \beta q(\frac{a+b}{2}) + \gamma q(b),
  \end{equation}
  We can determine the coefficients $\alpha$, $\beta$, and $\gamma$ such that this holds for any
  quadratic function $q(x)$. Further, this will then hold for $q(x)$ being cubic as well. Thus

  \begin{equation}
  \begin{split}
  \int_a^b f(x) dx &= \int_a^b f(a) + f^\prime(a) (x-a) + f^{\prime\prime}(a) (x-a)^2 + f^{(3)}(a) (x-a)^3 + \mathcal{O}\left((x-a)^4\right) dx \\
  &= h \left( \frac{1}{6} f(a) + \frac{4}{6} f(\frac{a+b}{2}) + \frac{1}{6} f(b) \right) + \mathcal{O}(h^5),
  \end{split}
  \end{equation}
  where $h = b-a$ and we used that $\alpha = \gamma = \frac{1}{6}$ and $\beta = \frac{4}{6}$.

  As before, the total error is $\mathcal{O}(h^4)$ when summing this rule over $N$ segments, where
  $h = \frac{b-a}{N}$.

  It should be noted that employing the refinement scheme outlined in section \ref{sec:trap} we
  obtain\cite{nr}:

  \begin{equation}
  I_m^{\text{Simpson}} = \frac{4}{3} I_{m+1}^{\text{Trapezoidal}} - \frac{1}{3} I_m^{\text{Trapezoidal}}.
  \end{equation}

  This is implemented in \texttt{pkg/quad/simp.go}, reusing the function for computing steps of the
  trapezoidal approximation.

  Higher order quadrature methods exist. E.g. Gaussian Quadrature, where for an $n^{th}$
  order method $f(x)$ is evaluated at the roots of the $n^{th}$ order Legendre Polynomial\cite{gauss}.
  Practical gains from such methods are negligible: for equation \ref{eq:target} we can easily achieve
  machine accuracy within fractions of a second (section \ref{sec:results}).

\subsection{Monte-Carlo Integration}
  \subsubsection{Importance Sampling}
  \label{sec:mon-is}

  We start by considering
  \begin{equation}
  \int_a^b f(x) dx = \int_{a}^{b} \frac{f(x)}{p(x)} p(x) dx = \mathbb{E}_{\sim p(x)}\left[ \frac{f(x)}{p(x)} \right],
  \end{equation}
  where $p(x)dx$ is a probability measure with support $[a, b]$ and $\mathbb{E}$ denotes the expectation.
  Computing the integral reduces to determining $\mathbb{E}\left[ \frac{f(x)}{p(x)} \right]$.

  In general, an unbiased estimator for $\mathbb{E}\left[ x \right]$ is
  \begin{equation}
  \bar{x}_N = \frac{1}{N} \sum_{n=1}^N x_n,
  \end{equation}
  with $x_n \sim p(x)$. Using the central limit theorem for \iid{} samples we can also provide the
  variance of our estimate. Specifically

  \begin{equation}
  \label{eq:var}
  \operatorname{var}(\bar{x}_N) = \frac{\operatorname{var}(x)}{N}.
  \end{equation}

  And given $N$ \iid{} samples the variance is estimated by
  \begin{equation}
  \bar{\sigma}_x^2 = \frac{1}{N-1} \sum_{n=1}^N (x_n - \bar{x}_N)^2,
  \end{equation}
  where we used Bessel's Correction\cite{nr} to obtain an unbiased estimator.

  From equation \ref{eq:var} the accuracy scales as $\sim \frac{1}{\sqrt{N}}$. Thus in general we will
  require many samples to find a good estimate. This means we seek to parallelize the algorithm and need
  to utilize an online estimator, as we will be memory-limited\footnotemark.

  \footnotetext{A quick calculation reveals that to attain an accuracy of $\sigma = 10^{-5}$ we would
  require about $10^{10}$ samples. Using 64 bit floating point numbers to store the results, this would
  require about $600$ GB of storage or $60\%$ of my computers total storage capacity.}

  A numerically stable online estimator for expectation and variance is given by
  \begin{equation}
  \bar{x}_n = \bar{x}_{n-1} + \frac{x_n - \bar{x}_{n-1}}{n}
  \end{equation}
  and
  \begin{equation}
  M_{2,n} = M_{2,n-1} + (x_n - \bar{x}_{n-1})(x_n - \bar{x}_n),
  \end{equation}
  where $\bar{\sigma}_x^2 = \frac{1}{N-1} M_{2,n}$\cite{welford}.

  Further, given two estimates computed from sets $X_A$, $X_B$ of \iid{} samples the combined
  estimate is
  \begin{equation}
  \begin{split}
  \delta & = \bar{x}_B - \bar{x}_A, \\
  \bar{x}_N & = \bar{x}_A + \delta \frac{N_B}{N}, \\
  M_{2,N} & = M_{2,A} + M_{2,B} + \delta^2 \frac{N_A N_B}{N},
  \end{split}
  \end{equation}
  with $N = N_A + N_B$\cite{chan}.

  Note that this is numerically unstable for $N_A \approx N_B$ and both are large\cite{chan}. But as we
  either have $N_A \approx N_B$ and both are small, or $N_A \gg N_B$ we do not encounter this case.

  This is implemented in the \texttt{casino} package. The algorithm provides a facility to interactively
  refine an estimate using a number of concurrent workers. Each worker uses a separate \texttt{PCG XSL RR 128/64}
  random number generator. This is possible as \texttt{PCG} is very light-weight and computationally cheap\cite{pcg}.
  See appendix \ref{app:pcg} for more details.

  Using the expectation estimate \texttt{/pkg/quad/mont.go} implements integration using \is{}. For
  the \is{} integration we report the accuracy as $2\frac{\bar\sigma_N}{\sqrt N}$.

  The choice of proposal function is a very important factor in
  obtaining a small variance. Using a proposal function $\frac{f(x)}{p(x)} = constant$, we get
  $\operatorname{var}\left(\frac{f(x)}{p(x)}\right) = 0$. Unless we can integrate $p$\footnote{Which is
  equivalent to integrating $f$.}, we would need to utilize rejection based sampling. This is effectively
  equivalent to doing \is. Thus, we seek an integrable $p$ as close to $f$ as possible.

  \subsubsection{Adaptive Population Importance Sampling}
  The \apis{} algorithm is a generic algorithm for performing adaptive \is{}.

  The strategy of this class of algorithm is to use the data gathered during sampling to improve
  the efficiency of the proposal distribution, i.e. to reduce what we called
  $\operatorname{var}\left(\frac fp\right)$ in section \ref{sec:mon-is}.

  Specifically, the \apis{} algorithm estimates $I$, $Z$ for
  \begin{equation}
  I = \frac{1}{Z} \int_\chi f(\vec{x}) \pi(\vec{x}) d\vec{x},
  \end{equation}
  where $Z = \int_\chi \pi(\vec{x}) d\vec{x}$ is the partition function.

  To do so, a family of proposal distributions is introduced. We require that each proposal distribution
  $q_i$ is parameterized by its expectation\cite{apis}\footnotemark.

  \footnotetext{
    We could also adapt higher order moments, but only updating the first moment is the most numerically
    stable approach in general\cite{apis}.
  }

  Initially $q_i^{(t=0)}$ are chosen at random, or based on some knowledge of the problem. Then, for a number
  of iterations $T_a$ we perform \is{} to estimate $I$, $Z$, $\mu_i$ by drawing a one sample from each
  proposal distribution per iteration. Then we use $\mu_i$ to update $q_i^{(t)}$. This is repeated
  until the desired convergence criterion is meet. The estimates use online formulae very similar
  to what was discussed in \ref{sec:mon-is}.

  This algorithm is implemented in \texttt{/pkg/casino/apis.go} using families composed of Normal distributions.
  For more details on the algorithm and the precise method of computing the estimates, see\cite{apis}.

\section{Ensuring Implementation Correctness}
  \label{sec:tests}
  The source code for this project encompasses more than $3500$ lines of code\footnotemark. To ensure
  this code implements the algorithms in section \ref{sec:algo} correctly, we utilize unit-tests.
  Notice that this is separate from verifying specific results. Even when all algorithms are
  implemented correctly they may still fail to converge correctly for a number of reasons\cite{nr}.

  \footnotetext{
    As reported by \texttt{wc -l \$(find . -name "*.go")}.
  }

  Implementing these tests, we generally aim for $\ge 70\%$ coverage\footnote{Although
  we take the liberty to omit tests for some trivial parts of the code base}. By maintaining an extensive
  suite of tests we can also confidently re-factor code during the development process, getting immediate
  feedback for obvious mistakes. Tests can be found in files named \texttt{*\_test.go}.

  We test \texttt{casino} and \texttt{quad} routines using the same approach: knowing analytic
  results for some integral or expected value and variance, we compute estimates
  of these values. We then verify, that the integral values and accuracies reported are consistent with
  analytic results.

  The markers are invited to run tests and inspect the line-by-line test coverage themselves.
  For directions consult the \texttt{README.md} file.

\section{Results}
  \label{sec:results}
  To compute values for equation \ref{eq:target} we use the algorithms outlined in section \ref{sec:algo}.
  Quadrature methods are considered converged at a reported accuracy $\epsilon \le 10^{-6}$.
  For the \is{} schemes we progressively consider accuracies of $\epsilon \le 10^{-3},10^{-4},10^{-5},10^{-6}$.
  We compare two different proposal functions: a uniform distribution with support $[a,b] = [0,2]$ and
  a slanted distribution $\gamma(\alpha x + \beta)$ with $\alpha = -0.48$, $\beta = 0.98$, and
  $\gamma$ set to the appropriate normalizing factor for support $[a,b]$.
  For the \apis{} method we use $f(x) = 1$ with support $[a,b]$ and $\pi(x) = |\Psi(x)|^2$.

  The resulting values for the integral are given in figure \ref{fig:results}. All the computed values
  are in agreement and the most accurate result is \final. The accuracies are reported such that
  $x = \bar x \pm \epsilon$, since this is easier to compare than relative accuracies.

  Note that the accuracies obtained from \is{} are worse than for quadrature methods and require
  significantly more samples. This is due to the asymptotic behavior of the error $\sim \frac{1}{N^4}$
  (Simpson) and $\sim \frac{1}{\sqrt{N}}$ (\is). For higher dimensional integrals the error on quadrature
  methods is $\sim \frac{1}{N^{\frac4D}}$ (Simpson). Starting from $D=8$ asymptotic errors for \is{}
  are favorable. Our problem is $D=1$ so quadrature methods perform better as seen in figure \ref{fig:results}.

  The parallelized \is{} implementation takes around $25-35$ns per sample. To achieve $\epsilon \sim 10^{-5}$
  at $2\sigma$ confidence $\sim 10^{8}$ samples were required, taking $\sim 3\text{seconds}$. To improve
  by one order of magnitude and get the desired $\epsilon \sim 10^{-6}$, we expect $100\times$ the
  samples and runtime. This is indeed the case, as seen in the \is{} result taking
  $\sim 10\text{minutes}$\footnotemark.

  \footnotetext{
    Since the computations are completely independent, this could be cut this down by using multiple computers
    simultaneously. But for that one would need multiple computers. Or Google Cloud Credits. Or both.
    (Or credits with AWS, but I don't have those either.)
  }

  \apis{} is more sample efficient than \is{}, but is not implemented as a parallel algorithm and
  so has significantly longer run-time.

  \begin{figure}[ht]
  \centering
  \begin{tabular}{ l | l l l l }
    \textbf{Method} &
    \textbf{Value} &
    \textbf{Accuracy (reported)} &
    \textbf{Sample Count} &
    \textbf{Runtime} \\
    \hline

    Trapezoidal & $0.4976609222761679$  & $6.306742604267335\times10^{-7}$      & $257$         & $560\mu$s     \\
    Simpson     & $0.49766113237260257$ & $2.05317862711496\times10^{-9}$       & $129$         & $331\mu$s     \\

    Uniform \is & $0.49749275093080847$ & $0.0009987888600436882$               & $606208$      & $17.8$ms      \\
    Uniform \is & $0.4976236307864889$  & $9.9988154790576\times10^{-5}$        & $60522496$    & $1.62$s       \\
    Uniform \is & $0.497647454893018$   & $7.778787976918824\times10^{-5}$      & $99991552$    & $2.49$s       \\

    Slanted \is & $0.498257793433617$   & $0.0009384751322811317$               & $81920$       & $2.59$ms      \\
    Slanted \is & $0.4977119148814941$  & $9.99393644049716\times10^{-5}$       & $7274496$     & $212.29$ms    \\
    Slanted \is & $0.49765521455387013$ & $2.695543800022229\times10^{-5}$      & $99991552$    & $2.89$s       \\

    Slanted \is & $0.4976607485895988$  & $1.9059153608037185\times10^{-6}$     & $19999997952$ & $11$min $45$s \\

    \apis{}     & $0.4975202501301652$  & $0.00021774502243$ (see\footnotemark) & $606208$      & $18.14$s      \\

  \end{tabular}
  \caption{
    Results for different algorithms. All results agree to within the quoted accuracy.
    From these, we obtain a final value of \final{} (Simpson result).
  }
  \label{fig:results}
  \end{figure}

  \footnotetext{
    This value is based on the estimate for $Z \approx \bar Z = 1.0002177450224337$. Since
    $\pi = |\Psi|^2$ is normalized $\epsilon = |Z-\bar Z|$\cite{apis}.
  }

\section{Plausibility of Results}
\label{sec:verify}

  % Consider further:
  % - analyze numeric properties of quadrature method used and estimate numeric error incurred
  % - analyze numeric stability of monte-carlo method used

  All values reported (figure \ref{fig:results}) agree to within the quoted accuracy. Further, unit
  testing convinced us of the correctness of our implementation (section \ref{sec:tests}). We also note
  that the integrand is a symmetric, normalized distribution which decays exponentially in $x$.
  $I \approx \frac12$ is thus physically plausible.

  \subsection{Quadrature Methods}
  As seen in section \ref{sec:algo}, the Trapezoidal and Simpson's method have errors
  $\mathcal{O}\left( \frac{1}{N^2} \right)$ and $\mathcal{O}\left( \frac{1}{N^4} \right)$ respectively.

  This was for analytic functions and as $|\Psi|^2$ is analytic this analysis is valid
  for our results. Further one can show $|\Psi|^2$ is Lipschitz\footnote{For analytic functions this means
  a bounded derivative.}. Thus we expect excellent convergence
  from quadrature schemes which can be thought of as finite difference methods\footnote{They are special
  cases of the Runge-Kutta scheme\cite{nr}.}.

  This can also be seen in our reported errors, which follow the expected asymptotic behavior, shown in
  figures \ref{fig:trap} and \ref{fig:simp}.

  % Plots Quadrature
  \plot{proj-trap-accuracy}{
    Values obtained for the Trapezoidal method plotted against the number function evaluations required. The red
    error-bar graph shows the value computed for each step and the associated error, the green line
    shows the final converged value. The theoretical error bounds are shown in yellow. The reported errors
    follow the theoretical behavior very closely and all estimates lie within the theoretically expected range.
  }{fig:trap}

  \plot{proj-simp-accuracy}{
    Values obtained for the Simpson method plotted against the number function evaluations required. The red
    error-bar graph shows the value computed for each step and the associated error, the green line
    shows the final converged value. The theoretical error bounds are shown in yellow. The reported errors
    follow the theoretical behavior very closely and all estimates lie within the theoretically expected range.
  }{fig:simp}

  \subsection{Monte-Carlo Methods}
  As $|\Psi|^2$ is well-behaved, the expected value is well-defined and $= I$. As per our
  analysis in section \ref{sec:algo}, we expect the error on our estimate to behave like
  $\mathcal{O}\left( \frac{\operatorname{var}(\frac fp)}{\sqrt N} \right)$.

  It can further be noted that choosing $p$ to be more similar to $f$, in the sense of choosing $p$
  such that $\frac fp$ is bounded by the smallest possible interval, we decrease the variance and thus
  improve the error on our estimate by a constant factor.

  This theoretically expected behavior can clearly be seen in figures \ref{fig:mont-flat}, \ref{fig:mont-slant}.

  % Plots Monte-Carlo
  \plot{proj-mont-flat-accuracy}{
    Values obtained for the \is{} method (uniform proposal distribution) plotted against the number
    function evaluations required. The red
    error-bar graph shows the value computed for each step and the associated error, the green line
    shows the final converged value. The theoretical error bounds are shown in yellow. The reported errors
    follow the theoretical behavior very closely and all estimates lie within the theoretically expected range.
    All reported errors correspond to a $2\sigma$ confidence interval.
  }{fig:mont-flat}

  \plot{proj-mont-slanted-accuracy}{
    Values obtained for the \is{} method (slanted proposal distribution) plotted against the number
    function evaluations required. The red
    error-bar graph shows the value computed for each step and the associated error, the green line
    shows the final converged value. The theoretical error bounds are shown in yellow. The reported errors
    follow the theoretical behavior very closely and all estimates lie within the theoretically expected range.
    All reported errors correspond to a $2\sigma$ confidence interval. This graph nicely demonstrates the
    effect of choosing a proposal more similar to the integrand: compared with figure \ref{fig:mont-flat},
    the error bounds favorably scale by a constant factor.
  }{fig:mont-slant}

\bibliography{assignment.bib}{}
\bibliographystyle{plain}

\appendix{}

\section{Comparison with Tabulated Values}
\label{app:cheat}
From tables we obtain
\begin{equation}
I = \frac{\operatorname{erf}(2)}{2} = 0.49766113250947636708 \dots,
\end{equation}
where $\operatorname{erf}(x) = \frac{1}{\sqrt\pi}\int_{-x}^x e^{-t^2} dt$ is the Error function. Comparing
this to the our result \final one sees that the tabulated value is matched to within our
quoted error as expected based on the analysis in section \ref{sec:verify}.

\section{Permuted Linear Congruential Random Number Generators}
\label{app:pcg}

The PCG family of random number generators improves upon congruential generators by applying
permutations to the resulting bits\cite{pcg}. The general idea is to improve on linear congruential
generators of the form
\begin{equation}
S_{n+1} = \alpha S_n + \beta,
\end{equation}
where $S_n$, $\alpha$, and $\beta$ are part of a modular group under addition and multiplication.

We may notice, that the quality of the random bits produced increases as we consider higher bits
in the result\cite{pcg}. The idea of the PCG family of generators is to use this fact by
applying families of permutations to a subset of the produced bits, where the permutation family
members are selected `randomly' using the highest order bits.

The \texttt{PCG XSL RR 128/64} generator we used applies a rotation on an XOR shifted part of the
internal state, produces 64 bit random numbers, and uses 128 bits of internal state (giving a
period of $2^{128} \approx 3.40282\times10^{38}$)\cite{pcg}.

\end{document}
