\documentclass[10pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{fullpage}

\graphicspath{ {./images/} }

\title{Computational Physics Project \\Â Integrating Quantum Probability Densities}
\author{Tilman Roeder}
\date{\today}

% Uniform plots
\newcommand{\plot}[3]{\begin{figure}[ht]\centering\includegraphics[width=10cm]{#1}\caption{#2}\label{#3}\end{figure}}

% Abbreviations
\newcommand{\abbreviate}[3]{\newcommand{#1}{#3 \textit{(#2)}\renewcommand{#1}{#2}}}
\abbreviate{\apis}{APIS}{Adaptive Population Importance Sampling}
\abbreviate{\iid}{i.i.d.}{independent and identically distributed}
\abbreviate{\is}{IS}{Importance Sampling}


\newcommand{\final}{$I = 0.497661132 \pm 2\times10^{-9}$}

\begin{document}
\maketitle

\section{Introduction}
In this report we investigate methods for numerically computing the value of

\begin{equation}
\label{eq:target}
I = \int_a^b |\Psi(x)|^2 dx = \int_a^b \frac{1}{\sqrt{\pi}} e^{-x^2} dx,
\end{equation}
where we are interested in the case $a = 0$, $b = 2$.

$|\Psi(x)|^2$ is a normalized Gaussian probability density for which
very accurate approximations and tabulated values exist. However, for the purposes of this report we
treat the value of $I$ as unknown and investigate how to justify confidence in the
obtained results.

A direct comparison with tabulated values can be found in appendix \ref{app:cheat}.

\section{Algorithms}
\label{sec:algo}
We explore two general classes of algorithms: quadrature methods, and Monte-Carlo integration.

\subsection{Quadrature Methods}
  \subsubsection{Trapezoidal Rule}
  \label{sec:trap}
  Consider approximating the integrand $f(x)$ as a linear function going through $f(a)$, $f(b)$.
  Then, if $f$ is analytic, we have $f(x) = f_0 + f_0^\prime (x-x_0) + \mathcal{O}\left((x-x_0)^2\right)$
  and thus\footnotemark
  \begin{equation}
  \int_a^b f(x) dx =
  \int_a^b f(x_0) + \frac{f(b) - f(a)}{b-a} (x-a) + \mathcal{O}(x^2) dx =
  \frac{1}{2} h (f(a) + f(b)) + \mathcal{O}(h^3),
  \end{equation}
  where $h = b-a$ and $x_0 \in [a,b]$.

  \footnotetext{
    For analytic functions: $\forall a, b \in \mathbb{R} \exists x_0 \in [a, b]$ s.t.
    $f^\prime(x_0) = \frac{f(b)-f(a)}{b-a}$.
  }

  To compute the integral over a longer range we extend the rule by dividing the integration
  region into $N$ parts such that
  \begin{equation}
  \begin{split}
  I = \int_a^b f(x) dx & = \sum_{i=0}^{N-1} \int_{a+ih}^{a+(i+1)h} f(x) dx \\
  & = \sum_{i=0}^{N-1} \frac{1}{2} h (f(a+ih) + f(a+(i+1)h)) + \mathcal{O}(h^3) \\
  & = \frac{1}{2} h (f(a) + f(b)) + \left( \sum_{i=1}^{N-1} h f(a+ih) \right) + \mathcal{O}(h^2),
  \end{split}
  \end{equation}
  where $h = \frac{b-a}{N}$.

  Notice that given a desired accuracy $\epsilon$ in general $N \sim \frac{b-a}{\sqrt{\epsilon}}$
  function evaluations are necessary to determine the integral.

  % TODO: --- REVISE FROM HERE ---

  This can be implemented algorithmically, where we require $\mathcal{O}(N)$ time and $\mathcal{O}(1)$,
  space. However, making use of the strong support for parallel computing in modern CPUs and the Go
  programming language, we can reduce the time complexity of this algorithm by a factor of $M$, where $M$
  is the number of processes we can run concurrently\footnotemark.

  \footnotetext{This is true, since $M$ is limited by the number of cores in our processing unit. Given
  a theoretically infinite amount of truly concurrent processes, we would want to choose a scheme that
  combines two numbers at a time, leading to $\log_2(N)$ threads. It is clear that for any interesting
  use case this is limited by the number of processing cores available.}

  Further, we implement a scheme which progressively increases the number of steps taken until the
  approximation to the integral reaches a desired accuracy. This is achieved by sub-dividing
  the integration region, adding points half-way between the existing samples for every step \cite{nr}.

  Under this scheme, the integral can be computed as
  \begin{equation}
  \label{eq:trap-rec}
  I_m = \frac{I_m}{2} + \sum_i h_m f(x^{(m)}_i),
  \end{equation}
  where $m$ denotes the refinement step, $I_0 = \frac{h_0 (f(a) + f(b))}{2}$, $h_0 = b-a$,
  $h_m = \frac{h_{m-1}}{2}$, and $x^{(m)}_i$
  range over the subdivisions at the $m^{\text{th}}$ refinement stage.

  A quick way to estimate the error after a given step, is to compute $\epsilon_m \approx |I_m - I_{m-1}|$
  \cite{nr}.

  This algorithm is implemented in \texttt{pkg/quad/trap.go}. The implementation parallelizes the computation
  of the summation in equation \ref{eq:trap-rec}, which is implemented in \texttt{pkg/quad/trap\_step.go}.
  The \texttt{quad} package defines a generic \texttt{Integral} interface, which can be used to integrate
  a function, given a specific integration scheme. The constructor of the trapezoidal scheme permits the
  user to specify the number of worker routines desired when computing the successive steps of the integral.

  The algorithm used to parallelize the steps is built on the assumption that computation of the integrand
  $f(x)$ is in general non-trivial and the most expensive operation in the integration process. Thus, the
  algorithm has a worst case runtime of $\mathcal{O}(N)$, and a best case runtime of $\mathcal{O}(\frac{N}{M})$.

  \subsubsection{Simpson Rule}
  This quadrature method is closely related to the Trapezoidal Rule\footnotemark. We consider the integrand
  $f(x)$ to be approximated by a quadratic polynomial, passing through the function $f(x)$ at the points
  $a$, $\frac{a+b}{2}$, and $b$.

  \footnotetext{Indeed, we may understand both as special cases of the Runge-Kutta scheme for integrating
  ordinary first order differential equations\cite{nr}.}

  We can derive Simpson's rule by considering the following Ansatz:
  \begin{equation}
  \frac{1}{b-a} \int_a^b q(x) dx = \alpha q(a) + \beta q(\frac{a+b}{2}) + \gamma q(b),
  \end{equation}
  where $q(x)$ is a quadratic polynomial. And then determining the coefficients $\alpha$, $\beta$, and $\gamma$
  such that this holds for any $q(x)$. However, by choosing to evaluate $f(x)$ in the center of the interval,
  we find that this equation can be satisfied not just for second order polynomials $q(x)$, but also
  for all third order polynomials $q(x)$. Then:

  \begin{equation}
  \begin{split}
  \int_a^b f(x) dx &= \int_a^b f(a) + f^\prime(a) (x-a) + f^{\prime\prime}(a) (x-a)^2 + f^{(3)}(a) (x-a)^3 + \mathcal{O}\left((x-a)^4\right) dx \\
  &= h \left( \frac{1}{6} f(a) + \frac{4}{6} f(\frac{a+b}{2}) + \frac{1}{6} f(b) \right) + \mathcal{O}(h^5),
  \end{split}
  \end{equation}
  where $h = b-a$, and we have used that $\alpha = \gamma = \frac{1}{6}$ and $\beta = \frac{4}{6}$.

  As before, we obtain a total error of $\mathcal{O}(h^4)$, when summing this rule over a range subdivided
  into $N$ segments, with $h = \frac{b-a}{N}$.

  It should be noted that when employing the refinement scheme outlined in section \ref{sec:trap},
  one can obtain the following result\cite{nr}:

  \begin{equation}
  I_m^{\text{Simpson}} = \frac{4}{3} I_{m+1}^{\text{Trapezoidal}} - \frac{1}{3} I_m^{\text{Trapezoidal}}.
  \end{equation}

  Using this fact, the Simpson integration scheme is implemented in \texttt{pkg/quad/simp.go}. It
  reuses the function for computing trapezoidal integral approximations and thus directly benefits from
  the parallelized algorithm implemented for the evaluation under the Trapezoidal scheme, outlined in
  section \ref{sec:trap}.

\subsection{Monte-Carlo Integration}
  \subsubsection{Importance Sampling}
  We start by considering the integral
  \begin{equation}
  \int_a^b f(x) dx = \int_{a}^{b} \frac{f(x)}{p(x)} p(x) dx = \mathbb{E}_{\sim p(x)}\left[ \frac{f(x)}{p(x)} \right],
  \end{equation}
  where $p(x)dx$ is a probability measure with support $[a, b]$ and $\mathbb{E}$ denotes the expected
  value. We can see that we may compute any integral by determining the expected value
  $\mathbb{E}\left[ \frac{f(x)}{p(x)} \right]$.

  In general, we can estimate the expected value of a variate $x$ as:

  \begin{equation}
  \bar{x}_N = \frac{1}{N} \sum_{n=1}^N x_n,
  \end{equation}
  with $x_n \sim p(x)$, as $\mathbb{E}[\bar{x}_N] = \mathbb{E}[x]$. Using the central limit theorem for
  \iid{} samples we can also provide the variance on our estimate. Specifically, for \iid{} samples,
  we have

  \begin{equation}
  \label{eq:var}
  \operatorname{var}(\bar{x}_N) = \frac{\operatorname{var}(x)}{N}.
  \end{equation}

  Of course we can also estimate the variance given the set of $N$ \iid{} samples $x_n \sim p(x)$
  \begin{equation}
  \bar{\sigma}_x^2 = \frac{1}{N-1} \sum_{n=1}^N (x_n - \bar{x}_N)^2,
  \end{equation}
  where we used Bessel's Correction\cite{nr} to obtain an unbiased estimator for the variance such
  that
  \begin{equation}
  \mathbb{E}\left[\bar{\sigma}_x^2\right] = \operatorname{var}(x).
  \end{equation}

  As can be seen in equation \ref{eq:var}, the error on our estimate scales as $\sim \frac{1}{\sqrt{N}}$.
  Thus, we require to capture many random samples to achieve a good accuracy. Specifically, we would
  like to parallelize sampling, as one \iid{} sample can be taken and processed independently from
  all others. We also want to be able to interactively refine the estimate we have until we are
  satisfied with the error on $\bar{x}_N$.

  To this end, the algorithm implemented in \texttt{pkg/casino/expectation.go} does multiple things.
  The current state of the computation is encapsulated in a structure called \texttt{Expectation}.
  A consumer of this API can make consecutive calls to refine the estimates for $\bar{x}$ and
  $\operatorname{var}(x)$, until they are satisfied.

  Every for every refinement, the work load is split up into a specified number of runners, each of
  which computes an equal amount of \iid{} samples. Every runner uses a separate random number generator.
  These generators are reused in consecutive refinements and seeded once at initialization. The
  generator used is \texttt{PCG XSL RR 128/64}, which is a 64 bit generator from the PCG family\cite{pcg}.
  These generators are computationally cheap and light-weight, such that we can easily run a
  separate generator for every worker. See appendix \ref{app:pcg} for more details.

  To seed random number generators, the \texttt{casino} package provides a convenient list of random numbers
  generated from atmospheric noise.

  As we require to compute many samples, we are memory limited and need to utilize an online
  algorithm\footnotemark. Further, we are splitting the work to run over multiple workers, meaning we
  need to combine the expectation and variance estimates each worker produces.

  \footnotetext{A quick calculation reveals that to attain an accuracy of $\sigma = 10^{-5}$ we would
  require about $10^{10}$ samples. Using 64 bit floating point numbers to store the results, this would
  require about $600$ GB of storage or $60\%$ of my computers total storage capacity.}

  To compute numerically stable online estimates of the expectation and variance, we utilize the
  following recursive formulae
  \begin{equation}
  \bar{x}_n = \bar{x}_{n-1} + \frac{x_n - \bar{x}_{n-1}}{n}
  \end{equation}
  and

  \begin{equation}
  M_{2,n} = M_{2,n-1} + (x_n - \bar{x}_{n-1})(x_n - \bar{x}_n),
  \end{equation}
  where $\bar{\sigma}_x^2 = \frac{1}{N-1} M_{2,n}$ is the unbiased variance estimate\cite{welford}.

  When combining these estimates, we accumulate a global total estimate, which we successively combine
  with the individual online estimates as they become available. To combine two estimates, computed
  from the two sets $X_A$ and $X_B$ both containing \iid{}Â samples of $x$, we use the following relations\cite{chan}:
  \begin{equation}
  \begin{split}
  \delta & = \bar{x}_B - \bar{x}_A \\
  \bar{x}_N & = \bar{x}_A + \delta \frac{N_B}{N} \\
  M_{2,N} & = M_{2,A} + M_{2,B} + \delta^2 \frac{N_A N_B}{N}
  \end{split}
  \end{equation}

  It should be noted that this is numerically unstable in the case when $N_A \approx N_B$ and both
  are large\cite{chan}. However, we do not encounter this case. as the batches are generally small.
  Thus initially, we have $N_B = N_A$, but both are small, while later we have $N_A \gg N_B$.

  Note that the algorithm implemented in \texttt{/pkg/casino} does only compute the expectation of a
  statistic $x$. The \is{}, which is trivial to implement given a means of computing
  expected values and variances, is implemented by \texttt{/pkg/quad/mont.go}. It merely uses the
  expectation calculation to refine $\mathbb{E}\left[ \frac{f(x)}{p(x)} \right]$ until the desired
  accuracy is reached\footnote{Or the sample limit is exceeded.}.

  The algorithm implementing the \is{} integration, reports the accuracy as
  $2\frac{\bar\sigma_N}{\sqrt N}$.

  It should also be mentioned, that the choice of proposal function is a very important factor in
  obtaining a small variance on the final result. We could try to sample from a distribution which
  makes $\frac{f(x)}{p(x)} = constant$, such that we have
  $\operatorname{var}\left(\frac{f(x)}{p(x)}\right) = 0$, but then we would need to use a rejection
  based sampling algorithm\footnotemark. This would then increase the amount of entropy needed for our result,
  as well as decreasing any computational gains through the need of repeated sampling. Thus, the implemented
  algorithm only utilizes transformation based sampling methods.

  \footnotetext{Or know the integral already, which would allow us to to transformation based sampling.}

  \subsubsection{Adaptive Population Importance Sampling}
  The \apis{} algorithm is an generic algorithm for performing adaptive \is{}. The goal
  of this general class of algorithms is to adapt the proposal distribution used for computation, in
  order to improve the convergence rate. Here, we use \apis{} as an example of this more general class
  of algorithms.

  The basic idea behind \apis{} is to perform standard \is{}, for a number of epochs.
  Then utilize a statistic that we estimated alongside the \is{} process to adapt
  the family of proposal distributions to better resemble the target distribution\cite{apis}.

  Specifically, we try to evaluate
  \begin{equation}
  I = \frac{1}{Z} \int_\chi f(\vec{x}) \pi(\vec{x}) d\vec{x},
  \end{equation}
  where $Z = \int_\chi \pi(\vec{x}) d\vec{x}$ is the partition function.

  We start by randomly (or otherwise) initializing a set of proposal distributions $q_i^{(t=0)}$.
  For these proposal distributions we require that they are parameterized by their expected value\cite{apis}.
  The reason for only adapting the first order moments of the proposal distributions is that this is
  more stable than also adapting higher order moments\cite{apis}.

  Then, for every epoch $t$ we draw one sample $\vec{x}_i \sim q_i^{(t)}(\vec{x})$ from each
  of our proposal functions. These samples are then used to estimate $I$, $Z$, and $\mu_i$, the first
  order moments of the proposal distributions under $\pi$.

  After $n$ epochs, we update the proposal distributions $q_i^{(t)}$, using the estimate for $\mu_i$.
  We then repeat this procedure until convergence.

  This algorithm is implemented in \texttt{/pkg/casino}. For more details on the algorithm and the
  precise method of computing the estimates, see\cite{apis}.

\section{Ensuring Implementation Correctness}
  The source code for this project encompasses more than $3000$ lines of code. We want to ensure that
  this code implements the algorithms outlined in section \ref{sec:algo} correctly. Notice that this is separate
  from verifying the specific results, as even when all algorithms are implemented correctly we may
  still fail to converge to a correct result for a number of reasons\cite{nr}.

  To ensure implementation correctness, we utilize a large number of unit tests. When implementing these
  tests, we generally aim to achieve $\ge 70\%$ total coverage\footnote{Although we take the liberty
  to omit tests for some trivial parts of the code base}. By maintaining an extensive suite of tests,
  we can also confidently re-factor our source code during the development process and get immediate
  feedback if obvious mistakes are made.

  We test the expectation and integral routines using the same general approach: knowing the analytic
  results for some integral or expected value and variance, we run our routines to compute estimates
  of these values. We can then verify, that the integral values and accuracies reported match the
  analytic results.

  The markers are invited to run these tests and inspect the line-by-line test coverage themselves.
  For directions please consult the \texttt{README.md} file.

\section{Results}
  To compute values for equation \ref{eq:target}, we use the algorithms outlined in section \ref{sec:algo}.
  For the quadrature methods we consider the process converged for a reported accuracy $\epsilon \le 10^{-6}$.

  Further, we consider \is{} methods using two separate proposal functions: a uniform distribution with
  support $[a,b] = [0,2]$ and a slanted distribution of the form $\gamma(\alpha x + \beta)$ with $\alpha = -0.48$,
  $\beta = 0.98$, and $\gamma$ set to the appropriate scaling factor for a support $[a,b]$.

  For the \apis{} method, we consider $f(x) = 1$ with support $[a,b]$ and $\pi(x) = |\Psi(x)|^2$.

  The resulting values for the integral are given in figure \ref{fig:results}. All the computed values
  are in agreement with each other and the most accurate value computed gives \final.

  Note that the accuracies on the values obtained from \is{} algorithms is much lower than for quadrature
  methods and requires significantly more samples to compute. This is due to the asymptotic behavior of
  the error as $\frac{1}{N^4}$ (Simpson) and $\frac{1}{\sqrt{N}}$ (\is). For higher dimensional integrals,
  the error on quadrature methods scales like $\frac{1}{N^{\frac4D}}$ (Simpson). Starting from $D=8$, we have
  better asymptotic errors for \is{} methods. Since our problem is $D=1$ quadrature methods are
  the far superior choice as seen in the results.

  The project outline asked to compute the value of $I$ to an accuracy of $\epsilon \le 10^{-6}$ for
  both quadrature and Monte-Carlo methods. As can be seen in the table below, we have not obtained
  any results from \is{} or other Monte-Carlo methods that has a $2\sigma$ confidence interval matching
  this accuracy. Our implementation takes around $25-30$ns per sample. This means that to achieve an
  accuracy of $\epsilon \le 10^{-6}$ we would need to run our algorithm for about $6-7$ hours\footnotemark.

  \footnotetext{
    Since the computations are completely independent, we could cut this down by using multiple computers
    simultaneously.
  }

  \begin{figure}[ht]
  \centering
  \begin{tabular}{ l | l l l l }
    \textbf{Method} & \textbf{Value} & \textbf{Accuracy (reported)} & \textbf{Sample Count} & \textbf{Runtime} \\
    \hline
    Trapezoidal & $0.4976609222761679$ & $6.306742604267335\times10^{-7}$ & $257$ & negligible \\
    Simpson & $0.49766113237260257$ & $2.05317862711496\times10^{-9}$ & $129$ & negligible \\

    Uniform \is & $0.49749275093080847$ & $0.0009987888600436882$ & $606208$ & $17.8$ms \\
    Uniform \is & $0.4976236307864889$ & $9.9988154790576\times10^{-05}$ & $60522496$ & $1.62$s \\
    Uniform \is & $0.497647454893018$ & $7.778787976918824\times10^{-05}$ & $99991552$ & $2.49$s \\

    Slanted \is & $0.498257793433617$ & $0.0009384751322811317$ & $81920$ & $2.59$ms \\
    Slanted \is & $0.4977119148814941$ & $9.99393644049716\times10^{-05}$ & $7274496$ & $212.29$ms \\
    Slanted \is & $0.49765521455387013$ & $2.6955438000222285\times10^{-05}$ & $99991552$ & $2.89$s \\

    \apis{} & $0.4996739724614837$ & $0.00538124$ (see footnote\footnotemark) & $4096$ & negligible \\
  \end{tabular}
  \caption{
    Results for different algorithms. All results agree with each other within their quoted accuracy.
    From these, we obtain a final value of \final. This value is obtained from
    the Simpson integration rule.
  }
  \label{fig:results}
  \end{figure}

  \footnotetext{
    This value is based on the reported estimate for $Z \approx \bar Z = 1.0053812358940526$. Since
    we know that $\pi = |\Psi|^2$ is normalized, we use $\epsilon = |Z-\bar Z|$.
  }

\section{Verification of Results}
\label{sec:verify}

% Consider:
% - analyze numeric properties of quadrature method used and estimate numeric error incurred
% - analyze numeric stability of monte-carlo method used

  All values reported in figure \ref{fig:results} agree with each other to within the quoted accuracy.
  Further, using unit-testing it was established that the algorithms outlined in section \ref{sec:algo}
  are implemented correctly. It can also be noted, that the integrand is a symmetric probability distribution
  which decays fairly quickly as $x$ increases. A value $I$ near $\frac12$ is thus plausible. Further,
  specific reasons the methods should converge with the given integral will be explored and the
  convergence rates of the algorithms will be compared with what is expected theoretically.

  \subsection{Quadrature Methods}
  As we saw in section \ref{sec:algo}, the Trapezoidal and Simpson's method have errors
  $\mathcal{O}\left( \frac{1}{N^2} \right)$ and $\mathcal{O}\left( \frac{1}{N^4} \right)$ respectively.

  This analysis was true for analytic functions and as $|\Psi|$ is analytic, this analysis is valid
  for our results as well. Further, it can be shown that $|\Psi|$ is also Lipschitz, which for analytic
  functions means the derivative is bounded. Thus we may expect excellent convergence from our
  quadrature schemes, which can be thought of as finite difference schemes.

  Considering the errors reported by our algorithm, we can see that they follow the expected asymptotic
  behavior very closely. This is shown in figures \ref{fig:trap} and \ref{fig:simp}.

  % Plots Quadrature
  \plot{proj-trap-accuracy}{
    Values obtained for the Trapezoidal method plotted against the number function evaluations required. The red
    error-bar graph shows the value computed for each step and the associated error, the green line
    shows the final converged value. The theoretical error bounds are shown in yellow. The reported errors
    follow the theoretical behavior very closely and all estimated lie within the theoretically expected range.
  }{fig:trap}

  \plot{proj-simp-accuracy}{
    Values obtained for the Simpson method plotted against the number function evaluations required. The red
    error-bar graph shows the value computed for each step and the associated error, the green line
    shows the final converged value. The theoretical error bounds are shown in yellow. The reported errors
    follow the theoretical behavior very closely and all estimated lie within the theoretically expected range.
  }{fig:simp}

  \subsection{Monte-Carlo Methods}
  As the integral $I$ is well-defined, we expect the expected value to converge. Further, as per our
  analysis in section \ref{sec:algo}, we expect the error on our estimate to behave like
  $\mathcal{O}\left( \frac{\operatorname{var}(\frac fp)}{\sqrt N} \right)$.

  It can further be noted that choosing $p$ to be more similar to $f$, in the sense of choosing $p$
  such that $\frac fp$ is bounded by the smallest possible interval, we decrease the variance and thus
  improve the error on our estimate by a constant factor.

  This theoretically expected behavior can clearly be seen in figures \ref{fig:mont-flat} and
  \ref{fig:mont-slant}.

  % Plots Monte-Carlo
  \plot{proj-mont-flat-accuracy}{
    Values obtained for the \is{} method (uniform proposal distribution) plotted against the number
    function evaluations required. The red
    error-bar graph shows the value computed for each step and the associated error, the green line
    shows the final converged value. The theoretical error bounds are shown in yellow. The reported errors
    follow the theoretical behavior very closely and all estimated lie within the theoretically expected range.
    All reported errors correspond to a $2\sigma$ confidence interval.
  }{fig:mont-flat}

  \plot{proj-mont-slanted-accuracy}{
    Values obtained for the \is{} method (slanted proposal distribution) plotted against the number
    function evaluations required. The red
    error-bar graph shows the value computed for each step and the associated error, the green line
    shows the final converged value. The theoretical error bounds are shown in yellow. The reported errors
    follow the theoretical behavior very closely and all estimated lie within the theoretically expected range.
    All reported errors correspond to a $2\sigma$ confidence interval. This graph nicely demonstrates the
    effect of choosing a proposal more similar to the integrand: compared with figure \ref{fig:mont-flat},
    the error bounds favorably scale by a constant factor.
  }{fig:mont-slant}

\bibliography{assignment.bib}{}
\bibliographystyle{plain}

\appendix{}

\section{Comparison with Tabulated Values}
\label{app:cheat}
From tables we obtain
\begin{equation}
I = \frac{\operatorname{erf}(2)}{2} = 0.49766113250947636708 \dots,
\end{equation}
where $\operatorname{erf}(x) = \frac{1}{\sqrt\pi}\int_{-x}^x e^{-t^2} dt$ is the Error function. Comparing
this to the our result \final one sees that the tabulated value is matched to within our
quoted error, as expected based on the analysis in section \ref{sec:verify}.

\section{Permuted Linear Congruential Random Number Generators}
\label{app:pcg}

The PCG family of random number generators improves upon congruential generators by applying
permutations to the resulting bits\cite{pcg}. The general idea is to improve on linear congruential
generators of the form
\begin{equation}
S_{n+1} = \alpha S_n + \beta,
\end{equation}
where $S_n$, $\alpha$, and $\beta$ are part of a modular group under addition and multiplication.

We may notice, that the quality of the random bits produced increases as we consider higher bits
in the result\cite{pcg}. The idea of the PCG family of generators is to use this fact by
applying families of permutations to a subset of the produced bits, where the permutation family
members are selected `randomly' using the highest oder bits.

The \texttt{PCG XSL RR 128/64} generator we used applies a rotation on an XOR shifted part of the
internal state, produces 64 bit random numbers, and uses 128 bits of internal state (giving a
period of $2^{128} \approx 3.40282\times10^{38}$)\cite{pcg}.

\end{document}
