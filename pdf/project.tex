\documentclass[10pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{fullpage}

\graphicspath{ {./images/} }

\title{Computational Physics Project \\Â Integrating Quantum Probability Densities}
\author{Tilman Roeder}
\date{\today}

% Uniform plots
\newcommand{\plot}[3]{\begin{figure}[ht]\centering\includegraphics[width=10cm]{#1}\caption{#2}\label{#3}\end{figure}}

% Abbreviations
\newcommand{\abbreviate}[3]{\newcommand{#1}{#3 \textit{(#2)}\renewcommand{#1}{#2}}}
\abbreviate{\apis}{APIS}{Adaptive Population Importance Sampling}
\abbreviate{\iid}{i.i.d.}{independent and identically distributed}


\begin{document}
\maketitle

\section{Introduction}
In this report we investigate methods for numerically computing the value of a proper integral.
The integral under scrutiny is

\begin{equation}
\label{eq:target}
I = \int_a^b |\Psi(x)|^2 dx = \int_a^b \frac{1}{\sqrt{\pi}} e^{-x^2} dx,
\end{equation}
where we are interested in the case $a = 0$ and $b = 2$.

This is, of course, a normalized Gaussian probability density, for which
very accurate approximations and tabulated values exist. However, for the purposes of this report, we shall
treat the value of the integral $I$ as unknown and investigate how to justify our confidence in the
obtained numerical results.

A comparison of our results with the tabulated values for this integral can be found in appendix
\ref{app:cheat}.

\section{Algorithms}
We will explore two general classes of algorithms: deterministic quadrature methods, and probabilistic
Monte-Carlo integration schemes.

\subsection{Quadrature Methods}
  \subsubsection{Trapezoidal Rule}
  \label{sec:trap}
  First, we will consider the trapezoidal quadrature method. Here, we approximate the integrand $f(x)$
  as a linear function between the two step points, going through $f(x)$ in both points.

  Then, if the function $f$ is analytic, we have $f(x) = f_0 + f_1 (x-x_0) + \mathcal{O}((x-x_0)^2)$\footnotemark
  and thus
  \begin{equation}
  \int_a^b f(x) dx \approx
  \int_a^b f(x) + \frac{f(b) - f(a)}{b-a} (x-a) + \mathcal{O}(x^2) dx =
  \frac{1}{2} h (f(a) + f(b)) + \mathcal{O}(h^3),
  \end{equation}
  where $h = b-a$ and $x_0 \in [a,b]$.

  \footnotetext{
    This follows, as for analytic functions $\forall a, b \exists x_0 \in [a, b]$ s.t.
    $f^\prime(x_0) = \frac{f(b)-f(a)}{b-a}$.
  }

  Now if we wish to compute the integral over a long range $[a, b]$, we may extend the rule by dividing
  the integration region into $N$ sub-integrals so that
  \begin{equation}
  \begin{split}
  I = \int_a^b f(x) dx & = \sum_{i=0}^{N-1} \int_{a+ih}^{a+(i+1)h} f(x) dx \\
  & = \sum_{i=0}^{N-1} \frac{1}{2} h (f(a+ih) + f(a+(i+1)h)) + \mathcal{O}(h^3) \\
  & = \frac{1}{2} h (f(a) + f(b)) + \left( \sum_{i=1}^{N-1} h f(a+ih) \right) + \mathcal{O}(h^2),
  \end{split}
  \end{equation}
  where $h = \frac{b-a}{N}$.

  Notice that in this case, given a desired accuracy $\epsilon$, we expect that $N \sim \frac{b-a}{\sqrt{\epsilon}}$,
  function evaluations should be necessary to determine the function at the desired accuracy, presuming
  arbitrarily precise arithmetic.

  This can be implemented algorithmically, where we require $\mathcal{O}(N)$ time and $\mathcal{O}(1)$,
  space. However, making use of the strong support for parallel computing in modern CPUs and the Go
  programming language, we can reduce the time complexity of this algorithm by a factor of $M$, where $M$
  is the number of processes we can run concurrently\footnotemark.

  \footnotetext{This is true, since $M$ is limited by the number of cores in our processing unit. Given
  a theoretically infinite amount of truly concurrent processes, we would want to choose a scheme that
  combines two numbers at a time, leading to $\log_2(N)$ threads. It is clear that for any interesting
  use case this is limited by the number of processing cores available.}

  Further, we implement a scheme which progressively increases the number of steps taken until the
  approximation to the integral reaches a desired accuracy. This is achieved by sub-dividing
  the integration region, adding points half-way between the existing samples for every step \cite{nr}.

  Under this scheme, the integral can be computed as
  \begin{equation}
  \label{eq:trap-rec}
  I_m = \frac{I_m}{2} + \sum_i h_m f(x^{(m)}_i),
  \end{equation}
  where $m$ denotes the refinement step, $I_0 = \frac{h_0 (f(a) + f(b))}{2}$, $h_0 = b-a$,
  $h_m = \frac{h_{m-1}}{2}$, and $x^{(m)}_i$
  range over the subdivisions at the $m^{\text{th}}$ refinement stage.

  A quick way to estimate the error after a given steps, is to compute $\epsilon_m \approx |I_m - I_{m-1}|$
  \cite{nr}.

  This algorithm is implemented in \texttt{pkg/quad/trap.go}. The implementation parallelizes the computation
  of the summation in equation \ref{eq:trap-rec}, which is implemented in \texttt{pkg/quad/trap\_step.go}.
  The \texttt{quad} package defines a generic \texttt{Integral} interface, which can be used to integrate
  a function, given a specific integration scheme. The constructor of the trapezoidal scheme permits the
  user to specify the number of worker routines desired when computing the successive steps of the integral.

  The algorithm used to parallelize the steps is built on the assumption that computation of the integrand
  $f(x)$ is in general non-trivial and the most expensive operation in the integration process. Thus, the
  algorithm has a worst case runtime of $\mathcal{O}(N)$, and a best case runtime of $\mathcal{O}(\frac{N}{M})$.

  \subsubsection{Simpson Rule}
  This quadrature method is closely related to the Trapezoidal Rule\footnotemark. We consider the integrand
  $f(x)$ to be approximated by a quadratic polynomial, passing through the function $f(x)$ at the points
  $a$, $\frac{a+b}{2}$, and $b$.

  \footnotetext{Indeed, we may understand both as special cases of the Runge-Kutta scheme for integrating
  ordinary first order differential equations\cite{nr}.}

  We can derive Simpson's rule by considering the following Ansatz:
  \begin{equation}
  \frac{1}{b-a} \int_a^b q(x) dx = \alpha q(a) + \beta q(\frac{a+b}{2}) + \gamma q(b),
  \end{equation}
  where $q(x)$ is a quadratic polynomial. And then determining the coefficients $\alpha$, $\beta$, and $\gamma$
  such that this holds for any $q(x)$. However, by choosing to evaluate $f(x)$ in the center of the interval,
  we find that this equation can be satisfied not just for second order polynomials $q(x)$, but also
  for all third order polynomials $q(x)$. Then:

  \begin{equation}
  \begin{split}
  \int_a^b f(x) dx &= \int_a^b f(a) + f^\prime(a) (x-a) + f^{\prime\prime}(a) (x-a)^2 + f^{(3)}(a) (x-a)^3 + \mathcal{O}\left((x-a)^4\right) dx \\
  &= h \left( \frac{1}{6} f(a) + \frac{4}{6} f(\frac{a+b}{2}) + \frac{1}{6} f(b) \right) + \mathcal{O}(h^5),
  \end{split}
  \end{equation}
  where $h = b-a$, and we have used that $\alpha = \gamma = \frac{1}{6}$ and $\beta = \frac{4}{6}$.

  As before, we obtain a total error of $\mathcal{O}(h^4)$, when summing this rule over a range subdivided
  into $N$ segments, with $h = \frac{b-a}{N}$.

  It should be noted that when employing the refinement scheme outlined in section \ref{sec:trap},
  one can obtain the following result\cite{nr}:

  \begin{equation}
  I_m^{\text{Simpson}} = \frac{4}{3} I_{m+1}^{\text{Trapezoidal}} - \frac{1}{3} I_m^{\text{Trapezoidal}}.
  \end{equation}

  Using this fact, the Simpson integration scheme is implemented in \texttt{pkg/quad/simp.go}. It
  reuses the function for computing trapezoidal integral approximations and thus directly benefits from
  the parallelized algorithm implemented for the evaluation under the Trapezoidal scheme, outlined in
  section \ref{sec:trap}.

\subsection{Monte-Carlo Integration}
  \subsubsection{Importance Sampling}
  We start by considering the integral
  \begin{equation}
  \int_a^b f(x) dx = \int_{a}^{b} \frac{f(x)}{p(x)} p(x) dx = \mathbb{E}_{\sim p(x)}\left[ \frac{f(x)}{p(x)} \right],
  \end{equation}
  where $p(x)dx$ is a probability measure with support $[a, b]$ and $\mathbb{E}$ denotes the expected
  value. We can see that we may compute any integral by determining the expected value
  $\mathbb{E}\left[ \frac{f(x)}{p(x)} \right]$.

  In general, we can estimate the expected value of a variate $x$ as:

  \begin{equation}
  \bar{x}_N = \frac{1}{N} \sum_{n=1}^N x_n,
  \end{equation}
  with $x_n \sim p(x)$, as $\mathbb{E}[\bar{x}_N] = \mathbb{E}[x]$. Using the central limit theorem for
  \iid{} samples we can also provide the variance on our estimate. Specifically, for \iid{} samples,
  we have

  \begin{equation}
  \label{eq:var}
  \operatorname{var}(\bar{x}_N) = \frac{\operatorname{var}(x)}{N}.
  \end{equation}

  Of course we can also estimate the variance given the set of $N$ \iid{} samples $x_n \sim p(x)$
  \begin{equation}
  \bar{\sigma}_x^2 = \frac{1}{N-1} \sum_{n=1}^N (x_n - \bar{x}_N)^2,
  \end{equation}
  where we used Bessel's Correction\cite{nr} to obtain an unbiased estimator for the variance such
  that
  \begin{equation}
  \mathbb{E}\left[\bar{\sigma}_x^2\right] = \operatorname{var}(x).
  \end{equation}

  As can be seen in equation \ref{eq:var}, the error on our estimate scales as $\sim \frac{1}{\sqrt{N}}$.
  Thus, we require to capture many random samples to achieve a good accuracy. Specifically, we would
  like to parallelize sampling, as one \iid{} sample can be taken and processed independently from
  all others. We also want to be able to interactively refine the estimate we have until we are
  satisfied with the error on $\bar{x}_N$.

  To this end, the algorithm implemented in \texttt{pkg/casino/expectation.go} does multiple things.
  The current state of the computation is encapsulated in a structure called \texttt{Expectation}.
  A consumer of this API can make consecutive calls to refine the estimates for $\bar{x}$ and
  $\operatorname{var}(x)$, until they are satisfied.

  Every for every refinement, the work load is split up into a specified number of runners, each of
  which computes an equal amount of \iid{} samples. Every runner uses a separate random number generator.
  These generators are reused in consecutive refinements and seeded once at initialization. The
  generator used is \texttt{PCG XSL RR 128/64}, which is a 64 bit generator from the PCG family\cite{pcg}.
  These generators are computationally cheap and light-weight, such that we can easily run a
  separate generator for every worker. See appendix \ref{app:pcg} for more details.

  To seed random number generators, the \texttt{casino} package provides a convenient list of random numbers
  generated from atmospheric noise.

  As we require to compute many samples, we are memory limited and need to utilize an online
  algorithm\footnotemark. Further, we are splitting the work to run over multiple workers, meaning we
  need to combine the expectation and variance estimates each worker produces.

  \footnotetext{A quick calculation reveals that to attain an accuracy of $\sigma = 10^{-5}$ we would
  require about $10^{10}$ samples. Using 64 bit floating point numbers to store the results, this would
  require about $600$ GB of storage.}

  To compute numerically stable online estimates of the expectation and variance, we utilize the
  following recursive formulae
  \begin{equation}
  \bar{x}_n = \bar{x}_{n-1} + \frac{x_n - \bar{x}_{n-1}}{n}
  \end{equation}
  and

  \begin{equation}
  M_{2,n} = M_{2,n-1} + (x_n - \bar{x}_{n-1})(x_n - \bar{x}_n),
  \end{equation}
  where $\bar{\sigma}_x^2 = \frac{1}{N-1} M_{2,n}$ is the unbiased variance estimate\cite{welford}.

  When combining these estimates, we accumulate a global total estimate, which we successively combine
  with the individual online estimates as they become available. To combine two estimates, computed
  from the two sets $X_A$ and $X_B$ both containing \iid{}Â samples of $x$, we use the relations\cite{chan}
  \begin{equation}
  \begin{split}
  \delta & = \bar{x}_B - \bar{x}_A \\
  \bar{x}_N & = \bar{x}_A + \delta \frac{N_B}{N} \\
  M_{2,N} & = M_{2,A} + M_{2,B} + \delta^2 \frac{N_A N_B}{N}
  \end{split}
  \end{equation}

  It should be noted that this is numerically unstable in the case when $N_A \approx N_B$ and both
  are large\cite{chan}. However, we do not encounter this case, as the batches are generally small and so initially,
  we have $N_B = N_A$, but both are small, while later we have $N_A \gg N_B$.

  Note that the algorithm implemented in \texttt{/pkg/casino} does only compute the expectation of a
  statistic $x$. The importance sampling, which is trivial to implement given a means of computing
  expected values and variances, is implemented by \texttt{/pkg/quad/mont.go}. It merely uses the
  expectation calculation to refine $\mathbb{E}\left[ \frac{f(x)}{p(x)} \right]$ until the desired
  accuracy is reached\footnote{Or the sample limit is exceeded.}.

  % TODO - Explain the implementation of expected value (and online mean, variance)
  % TODO - explain random number generator used + reference paper
  % TODO - explain set of random seeds used and where they originate from
  % TODO - mention parallel runtime implementation

  % TODO - explain that we use rejection and how it's a trade of from using transformation method
  %        - less entropy, less sampling; but worse variance 

  \subsubsection{Adaptive Population Importance Sampling}
  explain the \apis{} algorithm briefly, and reference paper

\section{Ensuring Implementation Correctness}

- explain unit tests
- outline tests run and test coverage

\section{Results}

- present results

- mention that $10^{-6}$ is impossible with MC, report average time for 1 function evaluation given
  full process parallelization and use this to calculate the order of magnitude of time taken to
  get to $10^{-6}$, also mention that this will be much more favorable for higher-dimensional integrals

\section{Verification of Results}

- show that integrand is smooth (possibly even Lipschitz), and proof/ explain why this leads to
  correct results in integral from quadrature method
  - analyze numeric error under perfect arithmetic for our integrand

- analyze numeric properties of quadrature method used and estimate numeric error incurred

- analyze numeric stability of monte-carlo method used

- outline variance on integral supplied and explain that we use a two-sigma error range to
  and how we expect that to behave

- compare results of different algorithms + show they agree
- have graphs of how integral values change w/ iterations
  - including errors for monte-carlo code
  - include theoretical "bounds", starting from the converged value

\bibliography{assignment.bib}{}
\bibliographystyle{plain}

\appendix{}

\section{Comparison with Tabulated Values}
\label{app:cheat}
From tables we obtain
\begin{equation}
I = \frac{\operatorname{erf}(2)}{2} = 0.49766113250947636708 \dots,
\end{equation}
where $\operatorname{erf}(x) = \frac{1}{\sqrt\pi}\int_{-x}^x e^{-t^2} dt$ is the Error function.

TODO: Compare result with tabulated values here ...

\section{Permuted Linear Congruential Random Number Generators}
\label{app:pcg}

The PCG family of random number generators improves upon congruential generators by applying
permutations to the resulting bits\cite{pcg}. The general idea is to improve on linear congruential
generators of the form
\begin{equation}
S_{n+1} = \alpha S_n + \beta,
\end{equation}
where $S_n$, $\alpha$, and $\beta$ are part of a modular group under addition and multiplication.

We may notice, that the quality of the random bits produced increases as we consider higher bits
in the result\cite{pcg}. The idea of the PCG family of generators is to use this fact by
applying families of permutations to a subset of the produced bits, where the permutation family
members are selected `randomly' using the highest oder bits.

The \texttt{PCG XSL RR 128/64} generator we used applies a rotation on an XOR shifted part of the
internal state, produces 64 bit random numbers, and uses 128 bits of internal state (giving a
period of $2^{128} \approx 3.40282\times10^{38}$)\cite{pcg}.

\end{document}
