\documentclass[10pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{fullpage}

\graphicspath{ {./images/} }

\title{Computational Physics Project \\Â Integrating Quantum Probability Densities}
\author{Tilman Roeder}
\date{\today}

% Uniform plots
\newcommand{\plot}[3]{\begin{figure}[ht]\centering\includegraphics[width=10cm]{#1}\caption{#2}\label{#3}\end{figure}}

% Abbreviations
\newcommand{\abbreviate}[3]{\newcommand{#1}{#3 \textit{(#2)}\renewcommand{#1}{#2}}}
\abbreviate{\apis}{APIS}{Adaptive Population Importance Sampling}
\abbreviate{\iid}{i.i.d.}{independent and identically distributed}
\abbreviate{\is}{IS}{Importance Sampling}


\newcommand{\final}{$I = 0.497661132 \pm 2\times10^{-9}$}

\begin{document}
\maketitle

\section{Introduction}
In this report we investigate methods for numerically computing the value of

\begin{equation}
\label{eq:target}
I = \int_a^b |\Psi(x)|^2 dx = \int_a^b \frac{1}{\sqrt{\pi}} e^{-x^2} dx,
\end{equation}
where we are interested in the case $a = 0$, $b = 2$.

$|\Psi(x)|^2$ is a normalized Gaussian probability density for which
very accurate approximations and tabulated values exist. However, for the purposes of this report we
treat the value of $I$ as unknown and investigate how to justify confidence in the
obtained results.

A direct comparison with tabulated values can be found in appendix \ref{app:cheat}.

\section{Algorithms}
\label{sec:algo}
We explore two general classes of algorithms: quadrature methods, and Monte-Carlo integration.

\subsection{Quadrature Methods}
  \subsubsection{Trapezoidal Rule}
  \label{sec:trap}
  Consider approximating the integrand $f(x)$ as a linear function going through $f(a)$, $f(b)$.
  Then, if $f$ is analytic, we have $f(x) = f_0 + f_0^\prime (x-x_0) + \mathcal{O}\left((x-x_0)^2\right)$
  and thus\footnotemark
  \begin{equation}
  \int_a^b f(x) dx =
  \int_a^b f(x_0) + \frac{f(b) - f(a)}{b-a} (x-a) + \mathcal{O}(x^2) dx =
  \frac{1}{2} h (f(a) + f(b)) + \mathcal{O}(h^3),
  \end{equation}
  where $h = b-a$ and $x_0 \in [a,b]$.

  \footnotetext{
    For analytic functions: $\forall a, b \in \mathbb{R} \exists x_0 \in [a, b]$ s.t.
    $f^\prime(x_0) = \frac{f(b)-f(a)}{b-a}$.
  }

  To compute the integral over a longer range we extend the rule by dividing the integration
  region into $N$ parts such that
  \begin{equation}
  \begin{split}
  I = \int_a^b f(x) dx & = \sum_{i=0}^{N-1} \int_{a+ih}^{a+(i+1)h} f(x) dx \\
  & = \sum_{i=0}^{N-1} \frac{1}{2} h (f(a+ih) + f(a+(i+1)h)) + \mathcal{O}(h^3) \\
  & = \frac{1}{2} h (f(a) + f(b)) + \left( \sum_{i=1}^{N-1} h f(a+ih) \right) + \mathcal{O}(h^2),
  \end{split}
  \end{equation}
  where $h = \frac{b-a}{N}$.

  Notice that given a desired accuracy $\epsilon$ in general $N \sim \frac{b-a}{\sqrt{\epsilon}}$
  function evaluations are necessary to determine the integral.

  Implementing this scheme requires $\mathcal{O}(N)$ time and $\mathcal{O}(1)$ space. However, making
  use of the strong support for parallel computing in modern CPUs and the Go programming language allows
  us to reduce the time complexity by a factor of $M$, where $M$ is the number of threads
  running concurrently\footnotemark.

  \footnotetext{This is true, since $M$ is limited by the number of cores in our processing unit. Given
  a theoretically infinite amount of truly concurrent threads we would want to choose a scheme that
  combines two numbers at a time, leading to $\log_2(N)$ threads and time complexity. It is clear
  that for any interesting integral this is limited by the number of processing cores available.}

  Further, we can implement a scheme to progressively increases the number of integration steps until
  a desired accuracy is reached. A straight-forward way to achieve this is to add mid-way points
  between the samples taken with every iteration:
  \begin{equation}
  \label{eq:trap-rec}
  I_m = \frac{I_{m-1}}{2} + \sum_i h_m f(x^{(m)}_i),
  \end{equation}
  where $m$ denotes the refinement step, $I_0 = h_0\frac{f(a) + f(b)}{2}$, $h_0 = b-a$,
  $h_m = \frac{h_{m-1}}{2}$, and $x^{(m)}_i$
  ranges over the subdivisions at the $m^{\text{th}}$ step.

  An efficient way to estimate the error after a given step, is to compute $\epsilon_m \approx |I_m - I_{m-1}|$
  \cite{nr}.

  This algorithm is implemented in \texttt{pkg/quad/trap.go}. The implementation parallelizes the computation
  of the summation in equation \ref{eq:trap-rec}, which is implemented in \texttt{pkg/quad/trap\_step.go}.
  The \texttt{quad} package defines a generic \texttt{Integral} interface, which the algorithm implements.

  The parallel algorithm computing the steps assumes that computing $f(x)$ is non-trivial, in the sense
  that is takes longer to compute than adding the result to the integral total. Thus, the
  algorithm has a worst case runtime of $\mathcal{O}(N)$, and best case runtime of $\mathcal{O}(\frac NM)$.

  \subsubsection{Simpson Rule}
  This quadrature method is closely related to the Trapezoidal Rule\footnotemark. Consider
  $f(x)$ to be approximated by a quadratic polynomial $q(x)$, such that $f(a) = q(a)$,
  $f(\frac{a+b}{2}) = q(\frac{a+b}{2})$, and $f(b) = q(b)$.

  \footnotetext{Indeed, we may understand both as special cases of the Runge-Kutta scheme for integrating
  ordinary first order differential equations\cite{nr}.}

  Now take the following Ansatz:
  \begin{equation}
  \frac{1}{b-a} \int_a^b q(x) dx = \alpha q(a) + \beta q(\frac{a+b}{2}) + \gamma q(b),
  \end{equation}
  We can determine the coefficients $\alpha$, $\beta$, and $\gamma$ such that this holds for any
  quadratic function $q(x)$. Further, this will then hold for $q(x)$ being cubic as well. Thus

  \begin{equation}
  \begin{split}
  \int_a^b f(x) dx &= \int_a^b f(a) + f^\prime(a) (x-a) + f^{\prime\prime}(a) (x-a)^2 + f^{(3)}(a) (x-a)^3 + \mathcal{O}\left((x-a)^4\right) dx \\
  &= h \left( \frac{1}{6} f(a) + \frac{4}{6} f(\frac{a+b}{2}) + \frac{1}{6} f(b) \right) + \mathcal{O}(h^5),
  \end{split}
  \end{equation}
  where $h = b-a$ and we used that $\alpha = \gamma = \frac{1}{6}$ and $\beta = \frac{4}{6}$.

  As before, the total error is $\mathcal{O}(h^4)$ when summing this rule over $N$ segments, where
  $h = \frac{b-a}{N}$.

  It should be noted that employing the refinement scheme outlined in section \ref{sec:trap} we
  obtain\cite{nr}:

  \begin{equation}
  I_m^{\text{Simpson}} = \frac{4}{3} I_{m+1}^{\text{Trapezoidal}} - \frac{1}{3} I_m^{\text{Trapezoidal}}.
  \end{equation}

  This is implemented in \texttt{pkg/quad/simp.go}, reusing the function for computing steps of the
  trapezoidal approximation.

\subsection{Monte-Carlo Integration}
  \subsubsection{Importance Sampling}
  \label{sec:mon-is}

  We start by considering
  \begin{equation}
  \int_a^b f(x) dx = \int_{a}^{b} \frac{f(x)}{p(x)} p(x) dx = \mathbb{E}_{\sim p(x)}\left[ \frac{f(x)}{p(x)} \right],
  \end{equation}
  where $p(x)dx$ is a probability measure with support $[a, b]$ and $\mathbb{E}$ denotes the expectation.
  Computing the integral reduces to determining $\mathbb{E}\left[ \frac{f(x)}{p(x)} \right]$.

  In general, an unbiased estimator for $\mathbb{E}\left[ x \right]$ is
  \begin{equation}
  \bar{x}_N = \frac{1}{N} \sum_{n=1}^N x_n,
  \end{equation}
  with $x_n \sim p(x)$. Using the central limit theorem for \iid{} samples we can also provide the
  variance of our estimate. Specifically

  \begin{equation}
  \label{eq:var}
  \operatorname{var}(\bar{x}_N) = \frac{\operatorname{var}(x)}{N}.
  \end{equation}

  And given $N$ \iid{} samples the variance is estimated by
  \begin{equation}
  \bar{\sigma}_x^2 = \frac{1}{N-1} \sum_{n=1}^N (x_n - \bar{x}_N)^2,
  \end{equation}
  where we used Bessel's Correction\cite{nr} to obtain an unbiased estimator.

  From equation \ref{eq:var} the accuracy scales as $\sim \frac{1}{\sqrt{N}}$. Thus in general we will
  require many samples to find a good estimate. This means we seek to parallelize the algorithm and need
  to utilize an online estimator, as we will be memory-limited\footnotemark.

  \footnotetext{A quick calculation reveals that to attain an accuracy of $\sigma = 10^{-5}$ we would
  require about $10^{10}$ samples. Using 64 bit floating point numbers to store the results, this would
  require about $600$ GB of storage or $60\%$ of my computers total storage capacity.}

  A numerically stable online estimator for expectation and variance is given by
  \begin{equation}
  \bar{x}_n = \bar{x}_{n-1} + \frac{x_n - \bar{x}_{n-1}}{n}
  \end{equation}
  and
  \begin{equation}
  M_{2,n} = M_{2,n-1} + (x_n - \bar{x}_{n-1})(x_n - \bar{x}_n),
  \end{equation}
  where $\bar{\sigma}_x^2 = \frac{1}{N-1} M_{2,n}$\cite{welford}.

  Further, given two estimates computed from sets $X_A$, $X_B$ of \iid{} samples the combined
  estimate is
  \begin{equation}
  \begin{split}
  \delta & = \bar{x}_B - \bar{x}_A, \\
  \bar{x}_N & = \bar{x}_A + \delta \frac{N_B}{N}, \\
  M_{2,N} & = M_{2,A} + M_{2,B} + \delta^2 \frac{N_A N_B}{N},
  \end{split}
  \end{equation}
  with $N = N_A + N_B$\cite{chan}.

  Note that this is numerically unstable for $N_A \approx N_B$ and both are large\cite{chan}. But as we
  either have $N_A \approx N_B$ and both are small, or $N_A \gg N_B$ we do not encounter this case.

  This is implemented in the \texttt{casino} package. The algorithm provides a facility to interactively
  refine an estimate using a number of concurrent workers. Each worker uses a separate \texttt{PCG XSL RR 128/64}
  random number generator. This is possible as \texttt{PCG} is very light-weight and computationally cheap\cite{pcg}.
  See appendix \ref{app:pcg} for more details.

  Using the expectation estimate \texttt{/pkg/quad/mont.go} implements integration using \is{}. For
  the \is{} integration we report the accuracy as $2\frac{\bar\sigma_N}{\sqrt N}$.

  The choice of proposal function is a very important factor in
  obtaining a small variance. Using a proposal function $\frac{f(x)}{p(x)} = constant$, we get
  $\operatorname{var}\left(\frac{f(x)}{p(x)}\right) = 0$. Unless we can integrate $p$\footnote{Which is
  equivalent to integrating $f$.}, we would need to utilize rejection based sampling. This is effectively
  equivalent to doing \is. Thus, we seek an integrable $p$ as close to $f$ as possible.

  % GO OVER FROM HERE
  --------------------------------------------------------------------------------------------------

  \subsubsection{Adaptive Population Importance Sampling}
  The \apis{} algorithm is a generic algorithm for performing adaptive \is{}.

  The strategy of this class of algorithms is to use the data gathered during sampling to improve
  the efficiency of the proposal distribution, i.e. to reduce what we called
  $\operatorname{var}\left(\frac fp\right)$ in section \ref{sec:mon-is}.

  Specifically, the \apis{} algorithm estimates $I$, $Z$ for
  \begin{equation}
  I = \frac{1}{Z} \int_\chi f(\vec{x}) \pi(\vec{x}) d\vec{x},
  \end{equation}
  where $Z = \int_\chi \pi(\vec{x}) d\vec{x}$ is the partition function.

  To do so, a family of proposal distributions is introduced. We require that each proposal distribution
  $q_i$ is parameterized by its expectation\cite{apis}\footnotemark.

  \footnotetext{
    We could also adapt higher order moments, but only updating the first moment is the most numerically
    stable approach in general\cite{apis}.
  }

  Initially $q_i$ are chosen at random, or based on some knowledge of the problem. Then, for a number
  of iterations $T_a$ we perform \is{} to estimate $I$, $Z$, $\mu_i$. Then we use $\mu_i$ to update
  $q_i$. This is then repeated until the desired convergence criterion is meet.

  --------------------------------------------------------------------------------------------------

  The goal
  of this general class of algorithms is to adapt the proposal distribution used for computation, in
  order to improve the convergence rate. Here, we use \apis{} as an example of this more general class
  of algorithms.

  The basic idea behind \apis{} is to perform standard \is{}, for a number of epochs.
  Then utilize a statistic that we estimated alongside the \is{} process to adapt
  the family of proposal distributions to better resemble the target distribution\cite{apis}.

  Specifically, we try to evaluate
  \begin{equation}
  I = \frac{1}{Z} \int_\chi f(\vec{x}) \pi(\vec{x}) d\vec{x},
  \end{equation}
  where $Z = \int_\chi \pi(\vec{x}) d\vec{x}$ is the partition function.

  We start by randomly (or otherwise) initializing a set of proposal distributions $q_i^{(t=0)}$.
  For these proposal distributions we require that they are parameterized by their expected value\cite{apis}.
  The reason for only adapting the first order moments of the proposal distributions is that this is
  more stable than also adapting higher order moments\cite{apis}.

  Then, for every epoch $t$ we draw one sample $\vec{x}_i \sim q_i^{(t)}(\vec{x})$ from each
  of our proposal functions. These samples are then used to estimate $I$, $Z$, and $\mu_i$, the first
  order moments of the proposal distributions under $\pi$.

  After $n$ epochs, we update the proposal distributions $q_i^{(t)}$, using the estimate for $\mu_i$.
  We then repeat this procedure until convergence.

  This algorithm is implemented in \texttt{/pkg/casino}. For more details on the algorithm and the
  precise method of computing the estimates, see\cite{apis}.

\section{Ensuring Implementation Correctness}
  The source code for this project encompasses more than $3000$ lines of code. We want to ensure that
  this code implements the algorithms outlined in section \ref{sec:algo} correctly. Notice that this is separate
  from verifying the specific results, as even when all algorithms are implemented correctly we may
  still fail to converge to a correct result for a number of reasons\cite{nr}.

  To ensure implementation correctness, we utilize a large number of unit tests. When implementing these
  tests, we generally aim to achieve $\ge 70\%$ total coverage\footnote{Although we take the liberty
  to omit tests for some trivial parts of the code base}. By maintaining an extensive suite of tests,
  we can also confidently re-factor our source code during the development process and get immediate
  feedback if obvious mistakes are made.

  We test the expectation and integral routines using the same general approach: knowing the analytic
  results for some integral or expected value and variance, we run our routines to compute estimates
  of these values. We can then verify, that the integral values and accuracies reported match the
  analytic results.

  The markers are invited to run these tests and inspect the line-by-line test coverage themselves.
  For directions please consult the \texttt{README.md} file.

\section{Results}
  To compute values for equation \ref{eq:target}, we use the algorithms outlined in section \ref{sec:algo}.
  For the quadrature methods we consider the process converged for a reported accuracy $\epsilon \le 10^{-6}$.

  Further, we consider \is{} methods using two separate proposal functions: a uniform distribution with
  support $[a,b] = [0,2]$ and a slanted distribution of the form $\gamma(\alpha x + \beta)$ with $\alpha = -0.48$,
  $\beta = 0.98$, and $\gamma$ set to the appropriate scaling factor for a support $[a,b]$.

  For the \apis{} method, we consider $f(x) = 1$ with support $[a,b]$ and $\pi(x) = |\Psi(x)|^2$.

  The resulting values for the integral are given in figure \ref{fig:results}. All the computed values
  are in agreement with each other and the most accurate value computed gives \final.

  Note that the accuracies on the values obtained from \is{} algorithms is much lower than for quadrature
  methods and requires significantly more samples to compute. This is due to the asymptotic behavior of
  the error as $\frac{1}{N^4}$ (Simpson) and $\frac{1}{\sqrt{N}}$ (\is). For higher dimensional integrals,
  the error on quadrature methods scales like $\frac{1}{N^{\frac4D}}$ (Simpson). Starting from $D=8$, we have
  better asymptotic errors for \is{} methods. Since our problem is $D=1$ quadrature methods are
  the far superior choice as seen in the results.

  The project outline asked to compute the value of $I$ to an accuracy of $\epsilon \le 10^{-6}$ for
  both quadrature and Monte-Carlo methods. As can be seen in the table below, we have not obtained
  any results from \is{} or other Monte-Carlo methods that has a $2\sigma$ confidence interval matching
  this accuracy. Our implementation takes around $25-30$ns per sample. This means that to achieve an
  accuracy of $\epsilon \le 10^{-6}$ we would need to run our algorithm for about $6-7$ hours\footnotemark.

  \footnotetext{
    Since the computations are completely independent, we could cut this down by using multiple computers
    simultaneously.
  }

  \begin{figure}[ht]
  \centering
  \begin{tabular}{ l | l l l l }
    \textbf{Method} & \textbf{Value} & \textbf{Accuracy (reported)} & \textbf{Sample Count} & \textbf{Runtime} \\
    \hline
    Trapezoidal & $0.4976609222761679$ & $6.306742604267335\times10^{-7}$ & $257$ & negligible \\
    Simpson & $0.49766113237260257$ & $2.05317862711496\times10^{-9}$ & $129$ & negligible \\

    Uniform \is & $0.49749275093080847$ & $0.0009987888600436882$ & $606208$ & $17.8$ms \\
    Uniform \is & $0.4976236307864889$ & $9.9988154790576\times10^{-05}$ & $60522496$ & $1.62$s \\
    Uniform \is & $0.497647454893018$ & $7.778787976918824\times10^{-05}$ & $99991552$ & $2.49$s \\

    Slanted \is & $0.498257793433617$ & $0.0009384751322811317$ & $81920$ & $2.59$ms \\
    Slanted \is & $0.4977119148814941$ & $9.99393644049716\times10^{-05}$ & $7274496$ & $212.29$ms \\
    Slanted \is & $0.49765521455387013$ & $2.6955438000222285\times10^{-05}$ & $99991552$ & $2.89$s \\

    \apis{} & $0.4975202501301652$ & $0.00021774502243365745$ (see footnote\footnotemark) & $606208$ & $18.14$s \\
  \end{tabular}
  \caption{
    Results for different algorithms. All results agree with each other within their quoted accuracy.
    From these, we obtain a final value of \final. This value is obtained from
    the Simpson integration rule.
  }
  \label{fig:results}
  \end{figure}

  \footnotetext{
    This value is based on the reported estimate for $Z \approx \bar Z = 1.0002177450224337$. Since
    we know that $\pi = |\Psi|^2$ is normalized, we use $\epsilon = |Z-\bar Z|$.
  }

\section{Verification of Results}
\label{sec:verify}

% Consider:
% - analyze numeric properties of quadrature method used and estimate numeric error incurred
% - analyze numeric stability of monte-carlo method used

  All values reported in figure \ref{fig:results} agree with each other to within the quoted accuracy.
  Further, using unit-testing it was established that the algorithms outlined in section \ref{sec:algo}
  are implemented correctly. It can also be noted, that the integrand is a symmetric probability distribution
  which decays fairly quickly as $x$ increases. A value $I$ near $\frac12$ is thus plausible. Further,
  specific reasons the methods should converge with the given integral will be explored and the
  convergence rates of the algorithms will be compared with what is expected theoretically.

  \subsection{Quadrature Methods}
  As we saw in section \ref{sec:algo}, the Trapezoidal and Simpson's method have errors
  $\mathcal{O}\left( \frac{1}{N^2} \right)$ and $\mathcal{O}\left( \frac{1}{N^4} \right)$ respectively.

  This analysis was true for analytic functions and as $|\Psi|$ is analytic, this analysis is valid
  for our results as well. Further, it can be shown that $|\Psi|$ is also Lipschitz, which for analytic
  functions means the derivative is bounded. Thus we may expect excellent convergence from our
  quadrature schemes, which can be thought of as finite difference schemes.

  Considering the errors reported by our algorithm, we can see that they follow the expected asymptotic
  behavior very closely. This is shown in figures \ref{fig:trap} and \ref{fig:simp}.

  % Plots Quadrature
  \plot{proj-trap-accuracy}{
    Values obtained for the Trapezoidal method plotted against the number function evaluations required. The red
    error-bar graph shows the value computed for each step and the associated error, the green line
    shows the final converged value. The theoretical error bounds are shown in yellow. The reported errors
    follow the theoretical behavior very closely and all estimated lie within the theoretically expected range.
  }{fig:trap}

  \plot{proj-simp-accuracy}{
    Values obtained for the Simpson method plotted against the number function evaluations required. The red
    error-bar graph shows the value computed for each step and the associated error, the green line
    shows the final converged value. The theoretical error bounds are shown in yellow. The reported errors
    follow the theoretical behavior very closely and all estimated lie within the theoretically expected range.
  }{fig:simp}

  \subsection{Monte-Carlo Methods}
  As the integral $I$ is well-defined, we expect the expected value to converge. Further, as per our
  analysis in section \ref{sec:algo}, we expect the error on our estimate to behave like
  $\mathcal{O}\left( \frac{\operatorname{var}(\frac fp)}{\sqrt N} \right)$.

  It can further be noted that choosing $p$ to be more similar to $f$, in the sense of choosing $p$
  such that $\frac fp$ is bounded by the smallest possible interval, we decrease the variance and thus
  improve the error on our estimate by a constant factor.

  This theoretically expected behavior can clearly be seen in figures \ref{fig:mont-flat} and
  \ref{fig:mont-slant}.

  % Plots Monte-Carlo
  \plot{proj-mont-flat-accuracy}{
    Values obtained for the \is{} method (uniform proposal distribution) plotted against the number
    function evaluations required. The red
    error-bar graph shows the value computed for each step and the associated error, the green line
    shows the final converged value. The theoretical error bounds are shown in yellow. The reported errors
    follow the theoretical behavior very closely and all estimated lie within the theoretically expected range.
    All reported errors correspond to a $2\sigma$ confidence interval.
  }{fig:mont-flat}

  \plot{proj-mont-slanted-accuracy}{
    Values obtained for the \is{} method (slanted proposal distribution) plotted against the number
    function evaluations required. The red
    error-bar graph shows the value computed for each step and the associated error, the green line
    shows the final converged value. The theoretical error bounds are shown in yellow. The reported errors
    follow the theoretical behavior very closely and all estimated lie within the theoretically expected range.
    All reported errors correspond to a $2\sigma$ confidence interval. This graph nicely demonstrates the
    effect of choosing a proposal more similar to the integrand: compared with figure \ref{fig:mont-flat},
    the error bounds favorably scale by a constant factor.
  }{fig:mont-slant}

\bibliography{assignment.bib}{}
\bibliographystyle{plain}

\appendix{}

\section{Comparison with Tabulated Values}
\label{app:cheat}
From tables we obtain
\begin{equation}
I = \frac{\operatorname{erf}(2)}{2} = 0.49766113250947636708 \dots,
\end{equation}
where $\operatorname{erf}(x) = \frac{1}{\sqrt\pi}\int_{-x}^x e^{-t^2} dt$ is the Error function. Comparing
this to the our result \final one sees that the tabulated value is matched to within our
quoted error as expected based on the analysis in section \ref{sec:verify}.

\section{Permuted Linear Congruential Random Number Generators}
\label{app:pcg}

The PCG family of random number generators improves upon congruential generators by applying
permutations to the resulting bits\cite{pcg}. The general idea is to improve on linear congruential
generators of the form
\begin{equation}
S_{n+1} = \alpha S_n + \beta,
\end{equation}
where $S_n$, $\alpha$, and $\beta$ are part of a modular group under addition and multiplication.

We may notice, that the quality of the random bits produced increases as we consider higher bits
in the result\cite{pcg}. The idea of the PCG family of generators is to use this fact by
applying families of permutations to a subset of the produced bits, where the permutation family
members are selected `randomly' using the highest order bits.

The \texttt{PCG XSL RR 128/64} generator we used applies a rotation on an XOR shifted part of the
internal state, produces 64 bit random numbers, and uses 128 bits of internal state (giving a
period of $2^{128} \approx 3.40282\times10^{38}$)\cite{pcg}.

\end{document}
